{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Up Environment and Imports\n",
    "Import torch, torchvision (if needed), numpy, and other required libraries. Detect and print available device (CPU/GPU) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check for available device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Paths and Hyperparameters\n",
    "Define configuration variables: path to weights.pth, dataset root paths, batch size, number of epochs, learning rate, weight decay, and output directories. Use Python variables so they can be easily changed in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths and hyperparameters\n",
    "\n",
    "# Path to the pre-trained model weights\n",
    "weights_path = \"weights.pth\"\n",
    "\n",
    "# Dataset root paths\n",
    "train_data_path = \"./data/train\"\n",
    "val_data_path = \"./data/val\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Output directory for saving models and logs\n",
    "output_dir = \"./output\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define / Load Model Architecture\n",
    "Recreate the exact model architecture used when weights.pth was generated. This may involve defining a custom nn.Module class or instantiating a standard torchvision model with the correct number of classes. Print model summary and number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define / Load Model Architecture\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "# Define the model architecture (example: ResNet18 with a custom output layer)\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        # Load a pre-trained ResNet18 model\n",
    "        self.base_model = models.resnet18(pretrained=False)\n",
    "        # Replace the fully connected layer to match the number of classes\n",
    "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)\n",
    "\n",
    "# Number of classes in the dataset (update as needed)\n",
    "num_classes = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = CustomResNet(num_classes=num_classes)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary and number of parameters\n",
    "summary(model, input_size=(3, 224, 224))  # Assuming input images are 3x224x224\n",
    "\n",
    "# Print total number of trainable parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model Weights from weights.pth\n",
    "Load the state_dict from weights.pth using torch.load, map to the correct device, and call model.load_state_dict. Handle common issues like missing or unexpected keys and optionally support loading from a checkpoint dict that includes optimizer state and epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights from weights.pth\n",
    "try:\n",
    "    # Load the state_dict from the weights file\n",
    "    checkpoint = torch.load(weights_path, map_location=device)\n",
    "    \n",
    "    # Check if the checkpoint contains a full state_dict or a dictionary with additional info\n",
    "    if isinstance(checkpoint, dict) and \"model_state_dict\" in checkpoint:\n",
    "        # Load model weights from the checkpoint\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(\"Model weights loaded successfully from checkpoint.\")\n",
    "        \n",
    "        # Optionally, load optimizer state and epoch if needed\n",
    "        if \"optimizer_state_dict\" in checkpoint:\n",
    "            optimizer_state_dict = checkpoint[\"optimizer_state_dict\"]\n",
    "            print(\"Optimizer state_dict found in checkpoint.\")\n",
    "        if \"epoch\" in checkpoint:\n",
    "            start_epoch = checkpoint[\"epoch\"]\n",
    "            print(f\"Resuming training from epoch {start_epoch}.\")\n",
    "    else:\n",
    "        # Load model weights directly\n",
    "        model.load_state_dict(checkpoint)\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The weights file '{weights_path}' was not found.\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: Missing key in state_dict - {e}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: Runtime error while loading state_dict - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset and Dataloaders\n",
    "Define dataset objects (e.g., custom torch.utils.data.Dataset or torchvision datasets) along with required transforms/augmentations. Then create DataLoader instances for training and validation sets with appropriate batch_size, shuffle, num_workers, and pin_memory settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset and Dataloaders\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define data transformations for training and validation datasets\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),  # Randomly crop and resize to 224x224\n",
    "        transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ]),\n",
    "    \"val\": transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize to 256\n",
    "        transforms.CenterCrop(224),  # Center crop to 224x224\n",
    "        transforms.ToTensor(),  # Convert to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_dataset = datasets.ImageFolder(root=train_data_path, transform=data_transforms[\"train\"])\n",
    "val_dataset = datasets.ImageFolder(root=val_data_path, transform=data_transforms[\"val\"])\n",
    "\n",
    "# Create DataLoader instances for training and validation\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,  # Shuffle training data\n",
    "    num_workers=4,  # Number of subprocesses for data loading\n",
    "    pin_memory=True  # Pin memory for faster data transfer to GPU\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,  # No need to shuffle validation data\n",
    "    num_workers=4,  # Number of subprocesses for data loading\n",
    "    pin_memory=True  # Pin memory for faster data transfer to GPU\n",
    ")\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Loss Function and Optimizer\n",
    "Instantiate the loss function (e.g., CrossEntropyLoss, MSELoss) and an optimizer (e.g., Adam, SGD) using the model parameters and configured hyperparameters. Optionally configure a learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=learning_rate, \n",
    "    weight_decay=weight_decay\n",
    ")\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, \n",
    "    step_size=7,  # Decay learning rate every 7 epochs\n",
    "    gamma=0.1     # Multiply learning rate by 0.1\n",
    ")\n",
    "\n",
    "# Print confirmation\n",
    "print(\"Loss function, optimizer, and scheduler defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Epoch Training Step\n",
    "Implement a train_one_epoch function that loops over the training DataLoader, moves batches to the device, performs forward pass, computes loss, backpropagates, applies optimizer.step, and tracks running loss and accuracy. Include gradient zeroing and optional gradient clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_one_epoch function\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, clip_grad=None):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "        # Move inputs and labels to the selected device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Optional gradient clipping\n",
    "        if clip_grad is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Compute accuracy\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += torch.sum(preds == labels).item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    print(f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Example usage\n",
    "train_one_epoch(model, train_loader, criterion, optimizer, device, clip_grad=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Training Loop with Checkpointing\n",
    "Implement a train function that calls train_one_epoch for multiple epochs, logs metrics, optionally steps the LR scheduler, and saves checkpoints (including model.state_dict, optimizer.state_dict, current epoch, and best metric) to disk so training can be resumed from weights.pth-like files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the full training loop with checkpointing\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs, output_dir, start_epoch=0, best_metric=None):\n",
    "    best_metric = best_metric or float('-inf')  # Initialize best metric if not provided\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        # Train for one epoch\n",
    "        train_loss, train_accuracy = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "        # Validate the model\n",
    "        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        # Log metrics\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # Step the learning rate scheduler if defined\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Save the best model based on validation accuracy\n",
    "        is_best = val_accuracy > best_metric\n",
    "        if is_best:\n",
    "            best_metric = val_accuracy\n",
    "            print(f\"New best metric: {best_metric:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"best_metric\": best_metric\n",
    "        }\n",
    "        checkpoint_path = os.path.join(output_dir, f\"checkpoint_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move inputs and labels to the selected device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Example usage of the training loop\n",
    "train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    output_dir=output_dir,\n",
    "    start_epoch=checkpoint.get(\"epoch\", 0) if \"epoch\" in checkpoint else 0,\n",
    "    best_metric=checkpoint.get(\"best_metric\", None) if \"best_metric\" in checkpoint else None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop and Metrics\n",
    "Implement an evaluate function that sets the model to eval mode, iterates over the validation/test DataLoader without gradient computation, computes loss and metrics (e.g., accuracy, F1), and prints or returns them. Use this after each epoch in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the validation/test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader for the validation/test dataset.\n",
    "        criterion (torch.nn.Module): Loss function.\n",
    "        device (torch.device): Device to run the evaluation on.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Average loss and metrics (accuracy, F1 score).\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Initialize variables for F1 score calculation\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            # Move inputs and labels to the selected device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update running loss\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Compute accuracy\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_predictions += torch.sum(preds == labels).item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            # Collect labels and predictions for F1 score\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Compute average loss and accuracy\n",
    "    avg_loss = running_loss / total_samples\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    # Compute F1 score\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# Example usage of the evaluate function\n",
    "val_loss, val_accuracy, val_f1 = evaluate(model, val_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
